\documentclass{article}
\usepackage{amsmath}
\setlength{\topmargin}{0.1in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\newtheorem{theorem}{Theorem}


\begin{document}
{\Large
\begin{center}
CS534 --- Homework Assignment 1 --- {Due Oct 8th in class, 2015}
\end{center}
}

\noindent
{\Large
\textbf{Written assignment
}}
\begin{enumerate}
\item Consider two coins, one is fair and one is not. The unfair coin has a 1/10 probability for head. Now you close your eyes and pick a random coin (each coin has a 50\% probability being picked), and toss it twice. Answer the following question.
    \begin{enumerate}
    \item What is the probability of the first toss being head?
    %Your solution here
    \item If both tosses are heads, what is the probability the fair coin is chosen?
    % Your solution here
    \end{enumerate}

\item We have three boxes colored Red, Blue and Green respectively. There are 3 apples and 6 oranges in the red box, 3 apples and 3 oranges in the green box, and 5 apples and 3 oranges in the blue box. Now we randomly select one of the boxes (with equal probability) and grab a fruit from the box. What is the probability that it is an apple? If the fruit that we got is an orange, what is the probability that we selected the green box?
    %Your solution here


\item (Weighted linear regression) In class when discussing linear regression, we assume that the Gaussian noise is independently identically distributed. Now we assume the noises $\epsilon_1, \epsilon_2, \cdots \epsilon_n$ are independent but each $\epsilon_m \sim N(0, \sigma_m^2)$, i.e., it has its own distinct variance.

     \begin{enumerate}
     \item Write down the log likelihood function of $\mathbf{w}$.
         %Your solution here

     \item Show that maximizing the log likelihood is equivalent to minimizing a weighted least square loss function $J(\mathbf{W}) = \sum_{m=1}^n a_m(\mathbf{w}^T\mathbf{x}_m-y_m)^2$, and express each $a_m$ in terms of $\sigma_m$.\\
    %Your solution here

     \item Derive a batch gradient descent algorithm for optimizing this objective.\\
     %Your solution here

     \item Derive a closed form solution to this optimization problem.
    %Your solution here

\end{enumerate}

\item (Maximum likelihood estimation.) A DNA sequence is formed using four bases Adenine(A), Cytosine(C), Guanine(G), and Thymine(T). We are interested in estimating the probability of each base appearing in a DNA sequence. Note that here we are treating each DNA position's base as a random variable $x$ following a categorical distribution of 4 values and assume the different sequences are generated by repeatedly sampling from this distribution. This distribution has 4 parameters, which we denote as $p_a, p_c, p_g$, and $p_t$ here. Given a collection of DNA sequences, please derive a maximum likelihood estimation for $p_a, p_c, p_g$, and $p_t$.

    Helpful Hint: the probability mass function for the discrete random variable can be written compactly as \[p(x) = \prod_{s=a,c,g,t}p_s^{I(x=s)}\]
Here $I(x=s)$ is an indicator function, and takes value 1 if $x$ is equal to $s$, and 0 otherwise.\\
    %Your solution here


\item A common way to get rid of having to deal with the bias
  separately on a perceptron is to add a new feature.  This feature
  always has value one, and you learn a weight for it.  Thus, if you
  have a $100$ dimensional problem with a bias, we solve it as a $101$
  dimensional problem without a bias.  Draw a picture for \emph{one
    dimensional} data and a linear separator with a (non-zero) bias
  and draw the corresponding picture for the same data, ``lifted''
  into two dimensions, with the corresponding lineear separate
  \emph{without} a bias.  (Please make sure that the two separators
  are actually \emph{equiavalent}!)
   %Your solution here

  \item The perceptron will only converge if the data is linearly
  separable. It is possible to \emph{force} your data to be linearly
  separable as follows.  If you have $N$ data points in $D$
  dimensions, map data point $\vec x_n$ to the $(D+N)$-dimensional
  point $\langle \vec x_n, e_n \rangle$, where $e_n$ is a
  $N$-dimensional vector of zeros, except for the $n$th position,
  which is $1$.  (Eg., $e_4 = \langle 0, 0, 0, 1, 0, \dots \rangle$.)

\begin{enumerate}
\item Show that if you apply this mapping the data becomes
  linearly separable (you may wish to do so by providing a weight
  vector $\vec w$ in $(D+N)$-dimensional space that successfully
  separates the data).
 %Your solution here

\item How does this mapping affect generalization?
 %Your solution here


\end{enumerate}
\end{enumerate}

\end{document}
